---
title: 23 计算神经网络的输出
date: 2025-12-17T19:43:01Z
lastmod: 2025-12-17T19:43:44Z
categories: 神经网络与深度学习
---

### 1. 神经网络结构回顾

- 考虑一个 **具有一个隐藏层** 的神经网络：

  - 输入层：3 个特征 → $\mathbf{x} = [x_1, x_2, x_3]^\top \in \mathbb{R}^3$
  - 隐藏层：4 个神经元（节点）
  - 输出层：1 个神经元（用于二分类，输出 $\hat{y}$）

> 💡 注：这种网络被称为  **“两层神经网络”** （因为只计带参数的层：隐藏层 + 输出层）。

---

### 2. 单个神经元的计算（类比逻辑回归）

　　每个神经元执行两步计算：

1. **线性组合**：

    $$
    z = \mathbf{w}^\top \mathbf{x} + b
    $$
2. **激活函数（Sigmoid）** ：

    $$
    a = \sigma(z) = \frac{1}{1 + e^{-z}}
    $$

　　这与逻辑回归完全一致。神经网络就是将这一过程 **在多个神经元上重复**。

---

### 3. 隐藏层的逐节点计算（非向量化形式）

　　对隐藏层第 $i$ 个神经元（$i = 1,2,3,4$）：

$$
z^{[1]}_i = (\mathbf{w}^{[1]}_i)^\top \mathbf{x} + b^{[1]}_i
$$

$$
a^{[1]}_i = \sigma(z^{[1]}_i)
$$

　　其中：

- 上标 $[1]$ 表示 **第 1 层（隐藏层）**
- 下标 $i$ 表示该层的第 $i$ 个神经元

---

### 4. 向量化：高效计算整个隐藏层

　　为了避免 for 循环，我们将所有神经元的计算 **向量化**。

#### 定义参数矩阵和向量：

- 权重矩阵 $\mathbf{W}^{[1]} \in \mathbb{R}^{4 \times 3}$：

  $$
  \mathbf{W}^{[1]} = 
  \begin{bmatrix}
  (\mathbf{w}^{[1]}_1)^\top \\
  (\mathbf{w}^{[1]}_2)^\top \\
  (\mathbf{w}^{[1]}_3)^\top \\
  (\mathbf{w}^{[1]}_4)^\top
  \end{bmatrix}
  $$
- 偏置向量 $\mathbf{b}^{[1]} \in \mathbb{R}^{4 \times 1}$：

  $$
  \mathbf{b}^{[1]} = 
  \begin{bmatrix}
  b^{[1]}_1 \\
  b^{[1]}_2 \\
  b^{[1]}_3 \\
  b^{[1]}_4
  \end{bmatrix}
  $$

#### 向量化前向传播（隐藏层）：

$$
\mathbf{z}^{[1]} = \mathbf{W}^{[1]} \mathbf{x} + \mathbf{b}^{[1]} \quad \in \mathbb{R}^{4 \times 1}
$$

$$
\mathbf{a}^{[1]} = \sigma(\mathbf{z}^{[1]}) \quad \text{（Sigmoid 逐元素作用）}
$$

> ✅ 尺寸验证：
>
> - $\mathbf{W}^{[1]}$: $4 \times 3$
> - $\mathbf{x}$: $3 \times 1$
> - $\mathbf{b}^{[1]}$: $4 \times 1$
> - $\mathbf{z}^{[1]}, \mathbf{a}^{[1]}$: $4 \times 1$

---

### 5. 输出层的计算

　　输出层只有一个神经元，其参数为：

- $\mathbf{w}^{[2]} \in \mathbb{R}^{1 \times 4}$
- $b^{[2]} \in \mathbb{R}$

　　计算：

$$
z^{[2]} = \mathbf{w}^{[2]} \mathbf{a}^{[1]} + b^{[2]} \quad \in \mathbb{R}
$$

$$
a^{[2]} = \sigma(z^{[2]}) = \hat{y}
$$

> 🔁 注意：这里 $\mathbf{w}^{[2]}$ 是行向量，因此无需转置。

> 💡 类比：输出层等价于以 $\mathbf{a}^{[1]}$ 为输入的 **逻辑回归模型**。

---

### 6. 整体前向传播流程（单样本）

　　给定输入 $\mathbf{x} \in \mathbb{R}^3$，神经网络输出 $\hat{y}$ 的完整计算步骤为：

$$
\begin{aligned}
\mathbf{z}^{[1]} &= \mathbf{W}^{[1]} \mathbf{x} + \mathbf{b}^{[1]} \\
\mathbf{a}^{[1]} &= \sigma(\mathbf{z}^{[1]}) \\
z^{[2]} &= \mathbf{w}^{[2]} \mathbf{a}^{[1]} + b^{[2]} \\
a^{[2]} &= \sigma(z^{[2]}) = \hat{y}
\end{aligned}
$$

> ✅ 这就是 **4 行代码** 实现整个神经网络的前向传播！

---

### 7. 符号约定补充

- 输入可记作：$\mathbf{a}^{[0]} = \mathbf{x}$
- 最终输出：$\hat{y} = a^{[2]}$
- 激活函数 $\sigma(\cdot)$ 在向量上是 **逐元素应用**

---

### 8. 下一步：批量向量化（处理多个样本）

　　虽然本视频只讨论 **单个训练样本**，但实际训练中我们会将 $m$ 个样本堆叠成矩阵：

- $\mathbf{X} = [\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(m)}] \in \mathbb{R}^{3 \times m}$
- 相应地，$\mathbf{Z}^{[1]}, \mathbf{A}^{[1]}$ 等也变成 $4 \times m$ 矩阵

　　这将在下一节讲解，核心思想是 **用矩阵运算一次性计算所有样本**，极大提升效率。

---

## ✅ 总结要点

|概念|说明|
| ------| ------------------------------------|
|**神经元计算**|$z = \mathbf{w}^\top \mathbf{x} + b$, $a = \sigma(z)$|
|**向量化动机**|避免 for 循环，利用矩阵运算加速|
|**隐藏层输出**|$\mathbf{a}^{[1]} = \sigma(\mathbf{W}^{[1]} \mathbf{x} + \mathbf{b}^{[1]})$|
|**输出层**|本质是逻辑回归，输入为隐藏层激活值|
|**符号规范**|上标 $[l]$ 表示层，下标 $i$ 表示神经元|
