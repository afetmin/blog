---
title: dL／dz 的推导
date: 2025-12-08T08:21:25Z
lastmod: 2025-12-08T08:22:47Z
categories: 神经网络与深度学习
---

### 🎯 目标

　　推导在二分类逻辑回归（使用 sigmoid 激活函数 + 交叉熵损失）中，损失函数 $L$ 对线性输出 $z$ 的导数：

$$
\frac{dL}{dz} = a - y
$$

　　其中：

- $a = \sigma(z)$ 是 sigmoid 激活输出，
- $y \in \{0,1\}$ 是真实标签，
- $L = -\big[y \log(a) + (1 - y)\log(1 - a)\big]$ 是二元交叉熵损失。

---

### 🔗 推导步骤（使用链式法则）

　　根据链式法则：

$$
\frac{dL}{dz} = \frac{dL}{da} \cdot \frac{da}{dz}
$$

#### 第一步：计算 $\frac{dL}{da}$

　　损失函数：

$$
L = -\big( y \log a + (1 - y) \log(1 - a) \big)
$$

　　对 $a$ 求导：

$$
\frac{dL}{da} = -\left( \frac{y}{a} - \frac{1 - y}{1 - a} \right) = \frac{-y}{a} + \frac{1 - y}{1 - a}
$$

　　通分后化简：

$$
\frac{dL}{da} = \frac{a - y}{a(1 - a)}
$$

#### 第二步：计算 $\frac{da}{dz}$

　　因为 $a = \sigma(z)$，而 sigmoid 函数的导数为：

$$
\frac{d}{dz} \sigma(z) = \sigma(z)(1 - \sigma(z)) = a(1 - a)
$$

　　所以：

$$
\frac{da}{dz} = a(1 - a)
$$

#### 第三步：相乘得到 $\frac{dL}{dz}$

$$
\frac{dL}{dz} = \frac{a - y}{a(1 - a)} \cdot a(1 - a) = a - y
$$

　　✅ 最终结果：

$$
\boxed{\frac{dL}{dz} = a - y}
$$

---

### 💡 补充说明

- 在 Andrew Ng 的课程中，常将 $\frac{dL}{dz}$ 简记为 **​`dz`​**​，因此你会看到代码或笔记中写成 `dz = a - y`。
- 这个简洁的结果是 **sigmoid + 交叉熵损失** 组合的一个重要优点——梯度形式简单，训练更稳定。
