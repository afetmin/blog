---
title: 21 神经网络概述
date: 2025-12-17T19:39:31Z
lastmod: 2025-12-17T19:40:30Z
categories: 神经网络与深度学习
---

### 1. 回顾：逻辑回归（Logistic Regression）

　　在上一周的学习中，我们学习了**逻辑回归模型**，其计算流程如下：

- 输入特征向量：$\mathbf{x} \in \mathbb{R}^{n_x}$
- 参数：权重 $\mathbf{w} \in \mathbb{R}^{n_x}$，偏置 $b \in \mathbb{R}$
- 线性组合：

  $$
  z = \mathbf{w}^\top \mathbf{x} + b
  $$
- 激活函数（Sigmoid）：

  $$
  a = \hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}}
  $$
- 损失函数（单个样本）：

  $$
  \mathcal{L}(a, y) = -\left[ y \log a + (1 - y) \log(1 - a) \right]
  $$

　　这个过程可以用一个**计算图（computation graph）**  表示：

$$
\mathbf{x}, \mathbf{w}, b \rightarrow z \rightarrow a = \hat{y} \rightarrow \mathcal{L}
$$

---

### 2. 神经网络的基本思想

　　神经网络的核心思想是：**将多个“逻辑回归单元”堆叠起来**，形成多层结构。

> “You can form a neural network by stacking together a lot of little sigmoid units.”

#### 示例结构（含一个隐藏层）：

- 输入层：3 个特征 $x_1, x_2, x_3$ → 向量 $\mathbf{x} \in \mathbb{R}^3$
- 隐藏层：3 个神经元（每个都执行类似逻辑回归的计算）
- 输出层：1 个神经元 → 输出 $\hat{y}$

---

### 3. 神经网络的前向传播（Forward Propagation）

　　我们将使用**方括号上标**表示**网络的层数**（注意：不是训练样本索引！）：

- $[\ell]$ 表示第 $\ell$ 层（layer）
- $(i)$ 表示第 $i$ 个训练样本（如 $x^{(i)}$）

#### 第 1 层（隐藏层）：

　　设隐藏层有 $n^{[1]} = 3$ 个神经元。

- 权重矩阵：$\mathbf{W}^{[1]} \in \mathbb{R}^{3 \times 3}$（因为输入是 3 维）
- 偏置向量：$\mathbf{b}^{[1]} \in \mathbb{R}^{3}$
- 线性部分：

  $$
  \mathbf{z}^{[1]} = \mathbf{W}^{[1]} \mathbf{x} + \mathbf{b}^{[1]}
  $$
- 激活部分（逐元素 Sigmoid）：

  $$
  \mathbf{a}^{[1]} = \sigma(\mathbf{z}^{[1]}) = 
  \begin{bmatrix}
  \sigma(z_1^{[1]}) \\
  \sigma(z_2^{[1]}) \\
  \sigma(z_3^{[1]})
  \end{bmatrix}
  $$

#### 第 2 层（输出层）：

- 权重：$\mathbf{W}^{[2]} \in \mathbb{R}^{1 \times 3}$
- 偏置：$b^{[2]} \in \mathbb{R}$
- 线性部分：

  $$
  z^{[2]} = \mathbf{W}^{[2]} \mathbf{a}^{[1]} + b^{[2]}
  $$
- 激活（输出）：

  $$
  a^{[2]} = \hat{y} = \sigma(z^{[2]})
  $$

> 注意：$a^{[2]}$ 就是最终预测值 $\hat{y}$。

#### 总结前向传播流程：

$$
\mathbf{x} 
\overset{\mathbf{W}^{[1]}, \mathbf{b}^{[1]}}{\longrightarrow} 
\mathbf{z}^{[1]} 
\overset{\sigma}{\longrightarrow} 
\mathbf{a}^{[1]} 
\overset{\mathbf{W}^{[2]}, b^{[2]}}{\longrightarrow} 
z^{[2]} 
\overset{\sigma}{\longrightarrow} 
a^{[2]} = \hat{y}
\rightarrow \mathcal{L}(a^{[2]}, y)
$$

---

### 4. 损失函数

　　对于单个样本 $(\mathbf{x}, y)$，损失函数仍为：

$$
\mathcal{L}(a^{[2]}, y) = -\left[ y \log a^{[2]} + (1 - y) \log(1 - a^{[2]}) \right]
$$

---

### 5. 反向传播（Backpropagation）概览

　　为了训练神经网络，我们需要通过**反向传播**计算梯度，更新参数。

　　逻辑回归中的反向传播步骤为：

$$
\mathcal{L} \rightarrow da \rightarrow dz \rightarrow d\mathbf{w}, db
$$

　　在神经网络中，这一过程被**重复多次**，从输出层向输入层逐层回传：

#### 反向传播路径（以两层网络为例）：

$$
\mathcal{L} 
\rightarrow da^{[2]} 
\rightarrow dz^{[2]} 
\rightarrow d\mathbf{W}^{[2]}, db^{[2]} 
\rightarrow da^{[1]} 
\rightarrow dz^{[1]} 
\rightarrow d\mathbf{W}^{[1]}, d\mathbf{b}^{[1]}
$$

> 这些梯度将用于梯度下降法更新参数。

---

### 6. 关键符号说明

|符号|含义|
| ------| --------------------------|
|$x^{(i)}$|第 $i$ 个训练样本的输入特征|
|$y^{(i)}$|第 $i$ 个训练样本的真实标签|
|$[\ell]$|第 $\ell$ 层（layer index）|
|$\mathbf{W}^{[\ell]}$|第 $\ell$ 层的权重矩阵|
|$\mathbf{b}^{[\ell]}$|第 $\ell$ 层的偏置向量|
|$\mathbf{z}^{[\ell]}$|第 $\ell$ 层的线性输出|
|$\mathbf{a}^{[\ell]}$|第 $\ell$ 层的激活输出（$\mathbf{a}^{[0]} = \mathbf{x}$）|

---

### 7. 核心思想总结

- **神经网络 = 多层逻辑回归单元的堆叠**
- 每一层都包含：**线性变换 + 非线性激活（如 Sigmoid）**
- 前向传播：从输入到输出逐层计算
- 反向传播：从损失函数反向计算梯度，用于参数更新
- **方括号上标** **$[\cdot]$** **表示层，圆括号上标** **$(\cdot)$** **表示样本**
