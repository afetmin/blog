---
title: 09 导数（Derivatives）的直观理解与实例
date: 2025-12-08T07:42:34Z
lastmod: 2025-12-08T07:42:49Z
categories: 神经网络与深度学习
---

### 一、导数的本质：函数在某一点的“斜率”

- **导数**（Derivative）描述的是函数在某一点处的**瞬时变化率**，也就是该点处切线的**斜率**。
- 对于函数 $f(a)$，其在点 $a$ 处的导数记作：

  $$
  \frac{d}{da} f(a)
  $$

  或简写为 $f'(a)$。

> ✅ **关键理解**：导数不是固定值（除非函数是直线），它会随着输入 $a$ 的不同而变化。

---

### 二、经典例子 1：$f(a) = a^2$

#### 1. 在 $a = 2$ 处

- $f(2) = 2^2 = 4$
- 微小扰动：$a = 2.001 \Rightarrow f(2.001) \approx 4.004001 \approx 4.004$
- 变化量：

  - $\Delta a = 0.001$
  - $\Delta f \approx 0.004$
- 斜率（导数）近似为：

  $$
  \frac{\Delta f}{\Delta a} \approx \frac{0.004}{0.001} = 4
  $$

#### 2. 在 $a = 5$ 处

- $f(5) = 25$
- $f(5.001) \approx 25.010005 \approx 25.010$
- $\Delta f \approx 0.010$，所以：

  $$
  \frac{\Delta f}{\Delta a} \approx \frac{0.010}{0.001} = 10
  $$

#### 3. 通用导数公式（来自微积分）：

$$
\frac{d}{da}(a^2) = 2a
$$

- 验证：

  - 当 $a = 2$，导数 = $2 \times 2 = 4$
  - 当 $a = 5$，导数 = $2 \times 5 = 10$

> 🔍 **注意**：使用 $\Delta a = 0.001$ 是近似，真正的导数定义基于**无穷小**（infinitesimal）变化。因此实际值（如 4.004001）与线性近似（4.004）之间存在微小误差。

---

### 三、经典例子 2：$f(a) = a^3$

- 微积分公式：

  $$
  \frac{d}{da}(a^3) = 3a^2
  $$
- 举例：$a = 2$

  - $f(2) = 8$
  - $f(2.001) = (2.001)^3 \approx 8.012018 \approx 8.012$
  - $\Delta f \approx 0.012$，$\Delta a = 0.001$
  - 斜率 ≈ $0.012 / 0.001 = 12$
- 公式验证：

  $$
  3a^2 = 3 \times 2^2 = 3 \times 4 = 12 \quad \checkmark
  $$

---

### 四、经典例子 3：$f(a) = \log(a)$（自然对数，底数为 $e$）

- 微积分公式：

  $$
  \frac{d}{da} \log(a) = \frac{1}{a}
  $$
- 举例：$a = 2$

  - $f(2) = \log(2) \approx 0.69315$
  - $f(2.001) \approx 0.69365$
  - $\Delta f \approx 0.0005$
  - 预期变化：$\frac{1}{a} \cdot \Delta a = \frac{1}{2} \times 0.001 = 0.0005 \quad \checkmark$

> 💡 这说明：当 $a = 2$ 时，函数增长速度只有输入变化的一半。

---

### 五、两个核心结论（Take-home Messages）

#### ✅ 结论 1：导数就是函数在某点的斜率

- 对于**线性函数**（如 $f(a) = 3a$），导数处处相同（恒为 3）。
- 对于**非线性函数**（如 $a^2, a^3, \log a$），导数随 $a$ 变化而变化。

#### ✅ 结论 2：常用导数公式可查表获得

- 不必每次都从定义推导，可直接使用微积分中的标准公式：

  $$
  \begin{aligned}
  \frac{d}{da}(a^n) &= n a^{n-1} \\
  \frac{d}{da}(\log a) &= \frac{1}{a} \\
  \frac{d}{da}(e^a) &= e^a \\
  \frac{d}{da}(\sin a) &= \cos a \quad \text{（虽未提及，但属常见）}
  \end{aligned}
  $$

---

### 六、后续预告：计算图（Computation Graph）

- 下一节将引入**计算图**（Computation Graph）的概念。
- 用于高效计算**复杂函数**（如神经网络中的损失函数）的导数。
- 为**反向传播**（Backpropagation）打下基础。

---

## 📌 总结表格：常见函数及其导数

|函数 $f(a)$|导数 $f'(a) = \frac{d}{da}f(a)$|
| -------------| -------|
|$a^2$|$2a$|
|$a^3$|$3a^2$|
|$\log a$|$\dfrac{1}{a}$|
|$c \cdot a$（$c$ 为常数）|$c$|

---

　　希望这份总结能帮助你牢固掌握导数的**直观意义**与**计算方法**，为后续学习神经网络的梯度下降和反向传播做好准备！
