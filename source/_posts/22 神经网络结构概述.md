---
title: 22 神经网络结构概述
date: 2025-12-17T19:40:58Z
lastmod: 2025-12-17T19:42:16Z
categories: 神经网络与深度学习
---

## 一、神经网络结构概述

　　本节讲解的是一个**具有单个隐藏层**的前馈神经网络（Feedforward Neural Network）。其结构如下：

- **输入层（Input Layer）** ：包含 3 个输入特征 $x_1, x_2, x_3$。
- **隐藏层（Hidden Layer）** ：包含 4 个神经元（节点）。
- **输出层（Output Layer）** ：包含 1 个神经元，输出预测值 $\hat{y}$。

> ⚠️ 注意：虽然从视觉上看有三层（输入、隐藏、输出），但**在神经网络术语中，输入层不计入层数**。因此该网络被称为  **“两层神经网络”** （Layer 1 = 隐藏层，Layer 2 = 输出层）。

---

## 二、符号约定（Notation）

　　为了清晰描述各层的计算过程，引入以下符号：

### 1. 输入表示

- 原始输入向量：

  $$
  \mathbf{x} = 
  \begin{bmatrix}
  x_1 \\ x_2 \\ x_3
  \end{bmatrix}
  \in \mathbb{R}^3
  $$
- 输入层的激活值（即输入本身）记为：

  $$
  \mathbf{a}^{[0]} = \mathbf{x}
  $$

  其中上标 $[0]$ 表示第 0 层（输入层），$\mathbf{a}$ 表示 **activation（激活值）** 。

### 2. 隐藏层激活值

- 隐藏层有 4 个神经元，其激活值构成一个 4 维向量：

  $$
  \mathbf{a}^{[1]} = 
  \begin{bmatrix}
  a^{[1]}_1 \\ a^{[1]}_2 \\ a^{[1]}_3 \\ a^{[1]}_4
  \end{bmatrix}
  \in \mathbb{R}^4
  $$

### 3. 输出层激活值

- 输出层只有一个神经元，其激活值即为模型预测：

  $$
  \hat{y} = a^{[2]} \in \mathbb{R}
  $$

---

## 三、“隐藏层”名称的由来

- 在**监督学习**中，训练数据提供的是输入 $\mathbf{x}$ 和真实标签 $y$。
- **隐藏层的激活值** **$\mathbf{a}^{[1]}$** **在训练集中不可见**（即没有标注），因此称为 “hidden”。
- 我们只能观测到输入和输出，中间的表示由模型自动学习。

---

## 四、参数（Weights and Biases）

　　每一层（除输入层外）都有对应的可学习参数：

### 1. 隐藏层参数（Layer 1）

- 权重矩阵 $\mathbf{W}^{[1]} \in \mathbb{R}^{4 \times 3}$：  
  每一行对应一个隐藏神经元，每一列对应一个输入特征。
- 偏置向量 $\mathbf{b}^{[1]} \in \mathbb{R}^{4 \times 1}$

> 为什么是 $4 \times 3$？  
> 因为有 4 个隐藏单元，每个接收 3 个输入。

### 2. 输出层参数（Layer 2）

- 权重向量 $\mathbf{W}^{[2]} \in \mathbb{R}^{1 \times 4}$
- 偏置标量 $b^{[2]} \in \mathbb{R}$（常写作 $1 \times 1$ 向量）

> 为什么是 $1 \times 4$？  
> 因为输出层有 1 个神经元，接收来自 4 个隐藏单元的输入。

---

## 五、前向传播计算流程（Forward Propagation）

　　整个网络的计算分为两步：

### 第一步：隐藏层计算

　　对隐藏层的每个神经元，先进行线性组合，再通过激活函数（如 sigmoid、ReLU 等）：

$$
\mathbf{z}^{[1]} = \mathbf{W}^{[1]} \mathbf{a}^{[0]} + \mathbf{b}^{[1]} \quad \in \mathbb{R}^4
$$

$$
\mathbf{a}^{[1]} = g^{[1]}(\mathbf{z}^{[1]})
$$

　　其中 $g^{[1]}$ 是隐藏层的激活函数（例如 ReLU）。

### 第二步：输出层计算

$$
z^{[2]} = \mathbf{W}^{[2]} \mathbf{a}^{[1]} + b^{[2]} \quad \in \mathbb{R}
$$

$$
\hat{y} = a^{[2]} = g^{[2]}(z^{[2]})
$$

　　若用于**二分类**，通常 $g^{[2]}$ 为 sigmoid 函数：

$$
\hat{y} = \sigma(z^{[2]}) = \frac{1}{1 + e^{-z^{[2]}}}
$$

---

## 六、关键总结

|概念|说明|
| ------| --------------------------------------------------------------------------|
|**层数命名**|输入层不计数，隐藏层为 Layer 1，输出层为 Layer 2 → 称为“两层神经网络”|
|**激活值**|$\mathbf{a}^{[l]}$ 表示第 $l$ 层的输出（传递给下一层的值）|
|**参数维度**|$\mathbf{W}^{[l]}$ 的形状为 $(n^{[l]}, n^{[l-1]})$，其中 $n^{[l]}$ 是第 $l$ 层神经元数量|
|**隐藏层含义**|其真实值在训练数据中不可见，需模型自行学习|

---

## 七、后续内容预告

　　下一讲将深入讲解**前向传播的具体计算过程**，包括：

- 如何逐层计算 $z$ 和 $a$
- 激活函数的选择与作用
- 参数维度的通用规则
