---
title: 24 多样本向量化（Vectorizing Across Multiple Examples）
date: 2025-12-17T19:43:52Z
lastmod: 2025-12-17T19:44:59Z
categories: 神经网络与深度学习
---

### 1. 背景回顾：单个样本的前向传播

　　在上一节课中，我们学习了如何对**单个训练样本** $x^{(i)}$ 进行前向传播，计算一个含**一个隐藏层**的神经网络的输出 $\hat{y}^{(i)} = a^{[2](i)}$。其计算步骤如下：

$$
\begin{aligned}
z^{[1](i)} &= W^{[1]} x^{(i)} + b^{[1]} \\
a^{[1](i)} &= \sigma\left(z^{[1](i)}\right) \\
z^{[2](i)} &= W^{[2]} a^{[1](i)} + b^{[2]} \\
a^{[2](i)} &= \sigma\left(z^{[2](i)}\right) = \hat{y}^{(i)}
\end{aligned}
$$

　　其中：

- $x^{(i)} \in \mathbb{R}^{n_x}$ 是第 $i$ 个训练样本；
- $W^{[l]}$ 和 $b^{[l]}$ 是第 $l$ 层的权重和偏置；
- $\sigma(\cdot)$ 是激活函数（如 sigmoid）；
- 上标 $[l]$ 表示第 $l$ 层，上标 $(i)$ 表示第 $i$ 个训练样本。

---

### 2. 问题：逐个处理效率低

　　若共有 $m$ 个训练样本，则朴素实现需循环 $m$ 次：

```python
for i in range(1, m+1):
    z[1](i) = W[1] @ x(i) + b[1]
    a[1](i) = sigma(z[1](i))
    ...
```

　　这种写法**计算效率低**，无法利用现代硬件（如 GPU）的并行能力。

---

### 3. 解决方案：向量化（Vectorization）

　　我们将所有训练样本**按列堆叠**成矩阵，从而一次性完成所有样本的前向传播。

#### 3.1 数据矩阵定义

- 输入矩阵：

  $$
  X = \begin{bmatrix} 
  | & | & & | \\
  x^{(1)} & x^{(2)} & \cdots & x^{(m)} \\
  | & | & & |
  \end{bmatrix} \in \mathbb{R}^{n_x \times m}
  $$
- 类似地，定义中间变量矩阵：

  $$
  Z^{[1]} = \begin{bmatrix} 
  | & | & & | \\
  z^{[1](1)} & z^{[1](2)} & \cdots & z^{[1](m)} \\
  | & | & & |
  \end{bmatrix} \in \mathbb{R}^{n^{[1]} \times m}
  $$

  $$
  A^{[1]} = \begin{bmatrix} 
  | & | & & | \\
  a^{[1](1)} & a^{[1](2)} & \cdots & a^{[1](m)} \\
  | & | & & |
  \end{bmatrix} \in \mathbb{R}^{n^{[1]} \times m}
  $$

> ✅ **关键约定**：
>
> - **水平方向（列）** ：不同训练样本（从左到右是样本 1 到 $m$）；
> - **垂直方向（行）** ：不同神经元（从上到下是第 1 个到第 $n^{[l]}$ 个隐藏单元）。

---

### 4. 向量化后的前向传播公式

　　将单样本公式中的小写字母（向量）替换为大写字母（矩阵），即可得到**完全向量化的批量计算**：

$$
\begin{aligned}
Z^{[1]} &= W^{[1]} X + b^{[1]} \\
A^{[1]} &= \sigma\left(Z^{[1]}\right) \\
Z^{[2]} &= W^{[2]} A^{[1]} + b^{[2]} \\
A^{[2]} &= \sigma\left(Z^{[2]}\right)
\end{aligned}
$$

> 🔍 注意：这里的加法 $W^{[1]}X + b^{[1]}$ 中，$b^{[1]}$ 是一个列向量（$\in \mathbb{R}^{n^{[1]} \times 1}$），但在 NumPy 等框架中会自动进行**广播（broadcasting）** ，将其加到每一列上，等价于对每个样本加上相同的偏置。

---

### 5. 广播机制说明（Broadcasting）

　　例如，设：

- $W^{[1]} \in \mathbb{R}^{n^{[1]} \times n_x}$
- $X \in \mathbb{R}^{n_x \times m}$
- $b^{[1]} \in \mathbb{R}^{n^{[1]} \times 1}$

　　则：

- $W^{[1]} X \in \mathbb{R}^{n^{[1]} \times m}$
- $b^{[1]}$ 会被广播为 $\mathbb{R}^{n^{[1]} \times m}$，每列都是 $b^{[1]}$

　　因此：

$$
Z^{[1]}[:, i] = W^{[1]} x^{(i)} + b^{[1]} \quad \text{对所有 } i=1,\dots,m
$$

　　这正是我们想要的！

---

### 6. 总结：向量化的优势

|方法|是否使用循环|计算效率|代码简洁性|是否适合 GPU|
| ----------| ----------------| ------------| ------------| --------------|
|朴素实现|是（for 循环）|低|差|❌|
|向量化|否|高（并行）|好|✅|

> 💡 **重要提示**：深度学习框架（如 TensorFlow、PyTorch）内部高度依赖此类向量化操作。掌握此技巧是写出高效、正确模型的基础。

---

### 7. 补充：符号规范回顾

|符号|含义|
| ------| --------------------------------|
|$x^{(i)}$|第 $i$ 个训练样本（列向量）|
|$X$|所有训练样本组成的矩阵（$n_x \times m$）|
|$a^{[l](i)}$|第 $i$ 个样本在第 $l$ 层的激活值|
|$A^{[l]}$|所有样本在第 $l$ 层的激活矩阵（$n^{[l]} \times m$）|
|$W^{[l]}$|第 $l$ 层的权重矩阵（$n^{[l]} \times n^{[l-1]}$）|
|$b^{[l]}$|第 $l$ 层的偏置向量（$n^{[l]} \times 1$）|

---

### ✅ 结语

　　通过将训练样本按列堆叠，并将单样本前向传播公式直接推广到矩阵形式，我们实现了**无循环、高效率的批量前向传播**。这一思想不仅适用于两层神经网络，也适用于更深的网络和更复杂的架构，是深度学习工程实践的核心技能之一。
